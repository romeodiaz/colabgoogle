{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/romeodiaz/colabgoogle/blob/main/%5BMAKE_A_COPY%5D_CVIP_%5BGE_AIVE%5D_ADK_Deploy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ADK Agent Deployment Notebook\n\nThis notebook deploys agents to Vertex AI Agent Engine for the `gc-aive` codebase.\n\n**Default deployment target:**\n1. `value_hypothesis_agent`\n\nCode can be loaded from a local path (`/Users/nathanm/Code/AIVE-20260209`) or cloned from GitHub (`Method360-CVIP/gc-aive`, branch `value_hypothesis_agent`).\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Authenticate GCP\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title 2. Configuration { display-mode: \"form\" }\n\n# @markdown ### GCP Project Settings\nPROJECT_ID = \"cvip-16562\"  # @param {type:\"string\"}\nDEPLOY_REGION = \"us-central1\"  # @param {type:\"string\"}\nMODEL_REGION = \"global\"  # @param {type:\"string\"}\nSTAGING_BUCKET = \"gs://adk_partnership_strategy_agent\"  # @param {type:\"string\"}\n# Impersonation is disabled; leave blank to use runtime default.\nIMPERSONATE_SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}\n# Leave empty unless you want to run as a custom service account.\nAGENT_SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}\n\n# @markdown ### Code Source\nSOURCE_MODE = \"local_path\"  # @param [\"local_path\", \"github_clone\"]\nLOCAL_REPO_PATH = \"/Users/nathanm/Code/AIVE-20260209\"  # @param {type:\"string\"}\n\n# @markdown ### GitHub Repository (used when SOURCE_MODE = \"github_clone\")\nGITHUB_REPO = \"https://github.com/Method360-CVIP/gc-aive\"  # @param {type:\"string\"}\nREPO_FOLDER = \"gc-aive\"  # @param {type:\"string\"}\nBRANCH_NAME = \"value_hypothesis_agent\"  # @param {type:\"string\"}\nGITHUB_TOKEN = \"\"  # @param {type:\"string\"}\n\n# @markdown ### Agent Deployment Settings\nAGENTS_DIR = \"agents\"  # @param {type:\"string\"}\nAGENT_DESCRIPTION = \"Value Hypothesis ADK Agent deployed via notebook\"  # @param {type:\"string\"}\n# Always create new Reasoning Engines (avoid sticky runtime SA)\nFORCE_CREATE_NEW_ENGINES = False  # @param {type:\"boolean\"}\n# Optional suffix for new engines (leave blank to auto-generate)\nDISPLAY_NAME_SUFFIX = \"\"  # @param {type:\"string\"}\n\n# @markdown ### Value Hypothesis Agent Settings (optional)\nOUTPUT_FOLDER_ID = \"\"  # @param {type:\"string\"}\nSALES_PLAYS_MD_FOLDER_ID = \"\"  # @param {type:\"string\"}\n\n# ---\n# Non-form configuration (edit in code view)\nAGENTS_TO_DEPLOY = [\"value_hypothesis_agent\"]\n# Add or remove agents in AGENTS_TO_DEPLOY as needed.\nUSE_AGENT_REQUIREMENTS = True\nBASE_REQUIREMENTS = [\"google-cloud-aiplatform[adk,agent_engines]\"]\n\nprint(\"Configuration:\")\nprint(f\"  Project: {PROJECT_ID}\")\nprint(f\"  Deploy Region: {DEPLOY_REGION}\")\nprint(f\"  Model Region: {MODEL_REGION}\")\nprint(f\"  Staging Bucket: {STAGING_BUCKET}\")\nprint(f\"  Source Mode: {SOURCE_MODE}\")\nprint(f\"  Local Repo Path: {LOCAL_REPO_PATH}\")\nprint(f\"  GitHub Repo: {GITHUB_REPO}\")\nprint(f\"  Branch: {BRANCH_NAME}\")\nprint(f\"  Impersonate SA: {IMPERSONATE_SERVICE_ACCOUNT or '(not set)'}\")\nprint(f\"  Agent SA: {AGENT_SERVICE_ACCOUNT or '(not set)'}\")\nprint(f\"  Force create new engines: {FORCE_CREATE_NEW_ENGINES}\")\nif FORCE_CREATE_NEW_ENGINES:\n    print(f\"  Display name suffix: {DISPLAY_NAME_SUFFIX or '(timestamp)'}\")\nprint(f\"  Use agent requirements: {USE_AGENT_REQUIREMENTS}\")\nprint(f\"  GitHub Token: {'***' if GITHUB_TOKEN else '(not set)'}\")\nprint(f\"  Agents to Deploy: {AGENTS_TO_DEPLOY}\")\nprint(f\"  Output Folder ID: {OUTPUT_FOLDER_ID or '(not set)'}\")\nprint(f\"  Sales Plays Folder ID: {SALES_PLAYS_MD_FOLDER_ID or '(not set)'}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 3. Prepare Source Code (Local Path or GitHub)\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Ensure we are in /content to avoid stale CWD issues\n",
    "CONTENT_DIR = \"/content\"\n",
    "os.makedirs(CONTENT_DIR, exist_ok=True)\n",
    "os.chdir(CONTENT_DIR)\n",
    "\n",
    "repo_path = os.path.join(CONTENT_DIR, REPO_FOLDER)\n",
    "\n",
    "# Clean up if repo already exists\n",
    "if os.path.exists(repo_path):\n",
    "    !rm -rf {repo_path}\n",
    "\n",
    "if SOURCE_MODE == \"local_path\":\n",
    "    print(f\"Using local path: {LOCAL_REPO_PATH}\")\n",
    "    if not os.path.exists(LOCAL_REPO_PATH):\n",
    "        raise FileNotFoundError(\n",
    "            f\"LOCAL_REPO_PATH not found: {LOCAL_REPO_PATH}. \"\n",
    "            \"If running in Colab, mount Drive first or set SOURCE_MODE to 'github_clone'.\"\n",
    "        )\n",
    "    shutil.copytree(LOCAL_REPO_PATH, repo_path)\n",
    "else:\n",
    "    if GITHUB_TOKEN:\n",
    "        print(\"Using authenticated clone (token provided)\")\n",
    "        # Configure git to use the token\n",
    "        !git config --global credential.helper store\n",
    "\n",
    "        # Write credentials to git credential store\n",
    "        with open(os.path.expanduser('~/.git-credentials'), 'w') as f:\n",
    "            f.write(f'https://x-access-token:{GITHUB_TOKEN}@github.com\\n')\n",
    "\n",
    "        !git clone {GITHUB_REPO} {repo_path}\n",
    "\n",
    "        # Clean up credentials file after clone\n",
    "        os.remove(os.path.expanduser('~/.git-credentials'))\n",
    "    else:\n",
    "        print(\"Using public clone (no token)\")\n",
    "        !git clone {GITHUB_REPO} {repo_path}\n",
    "\n",
    "    %cd {repo_path}\n",
    "    !git checkout {BRANCH_NAME}\n",
    "    %cd {CONTENT_DIR}\n",
    "\n",
    "%cd {repo_path}\n",
    "\n",
    "if os.path.exists('.git'):\n",
    "    print(\"\\nChecked out commit:\")\n",
    "    !git log --oneline -1\n",
    "else:\n",
    "    print(\"\\nUsing local snapshot (no .git directory found).\")\n",
    "\n",
    "# Verify agents directory exists\n",
    "if os.path.exists(AGENTS_DIR):\n",
    "    print(f\"\\n✓ Found agents directory: {AGENTS_DIR}\")\n",
    "    !ls -la {AGENTS_DIR}/\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Agents directory not found: {AGENTS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 4. Install Dependencies\n",
    "!pip install -q google-cloud-aiplatform[adk,agent_engines] google-adk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 5. Initialize Vertex AI\n",
    "import vertexai\n",
    "from vertexai import agent_engines\n",
    "\n",
    "vertexai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=DEPLOY_REGION,\n",
    "    staging_bucket=STAGING_BUCKET,\n",
    ")\n",
    "\n",
    "print(f\"✓ Vertex AI initialized\")\n",
    "print(f\"  Project: {PROJECT_ID}\")\n",
    "print(f\"  Location: {DEPLOY_REGION}\")\n",
    "print(f\"  Staging Bucket: {STAGING_BUCKET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title 6. Define Deployment Helper Functions\nimport os\nimport sys\nimport shutil\nimport tempfile\nimport importlib\nimport inspect\nimport subprocess\nimport re\nimport requests\nfrom typing import Optional, List, Dict, Any\nfrom datetime import datetime\nfrom google.api_core.exceptions import PermissionDenied\n\n\ndef get_access_token() -> str:\n    \"\"\"Get GCP access token.\"\"\"\n    token = !gcloud auth print-access-token\n    return token[0].strip()\n\n\ndef get_active_gcloud_account() -> Optional[str]:\n    \"\"\"Return active gcloud account if available.\"\"\"\n    try:\n        account = subprocess.check_output(\n            [\"gcloud\", \"config\", \"get-value\", \"account\"], text=True\n        ).strip()\n        return account if account and account != \"(unset)\" else None\n    except Exception:\n        return None\n\n\ndef _list_reasoning_engines() -> List[Dict[str, Any]]:\n    \"\"\"List reasoning engines with version fallback.\"\"\"\n    token = get_access_token()\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n    last_error = None\n\n    for api_version in (\"v1beta1\", \"v1\"):\n        url = (\n            f\"https://{DEPLOY_REGION}-aiplatform.googleapis.com/{api_version}\"\n            f\"/projects/{PROJECT_ID}/locations/{DEPLOY_REGION}/reasoningEngines\"\n        )\n        try:\n            response = requests.get(url, headers=headers)\n            if response.ok:\n                data = response.json() if response.text else {}\n                return data.get(\"reasoningEngines\", [])\n            last_error = f\"{response.status_code} {response.text[:200]}\"\n        except Exception as e:\n            last_error = str(e)\n\n    if last_error:\n        print(f\"  Warning: Could not query existing engines: {last_error}\")\n    return []\n\n\ndef get_existing_agent_engine_id(display_name: str) -> Optional[str]:\n    \"\"\"\n    Find existing agent engine by display name.\n    Returns the most recent engine ID if found, None otherwise.\n    \"\"\"\n    engines = _list_reasoning_engines()\n    matching = [e for e in engines if e.get(\"displayName\") == display_name]\n\n    if matching:\n        # Sort by createTime descending to get the most recent\n        matching.sort(key=lambda x: x.get(\"createTime\", \"\"), reverse=True)\n        engine_id = matching[0][\"name\"].split(\"/\")[-1]\n        return engine_id\n\n    return None\n\n\ndef extract_engine_resource(error_text: str) -> Optional[str]:\n    \"\"\"Extract reasoning engine resource path from an error string.\"\"\"\n    match = re.search(\n        r\"projects/\\d+/locations/[a-z0-9-]+/reasoningEngines/\\d+\",\n        error_text,\n    )\n    return match.group(0) if match else None\n\n\ndef print_engine_log_help(resource_name: str):\n    \"\"\"Print helpful log commands for a failed reasoning engine.\"\"\"\n    print(\"  Logs: open Cloud Logging and filter by this resource name:\")\n    print(f\"    {resource_name}\")\n    print(\"  Or run in a Colab cell:\")\n    engine_id = resource_name.split(\"/\")[-1]\n    cmd = (\n        \"!gcloud logging read \"\n        \"'resource.type=\\\"aiplatform.googleapis.com/ReasoningEngine\\\" \"\n        f\"AND resource.labels.reasoning_engine_id=\\\"{engine_id}\\\"' \"\n        \"--limit=50 --freshness=2h\"\n    )\n    print(f\"    {cmd}\")\n\n\ndef load_requirements_file(path: str) -> List[str]:\n    \"\"\"Load requirements.txt lines, stripping comments and blanks.\"\"\"\n    if not os.path.exists(path):\n        return []\n\n    requirements = []\n    with open(path, \"r\", encoding=\"utf-8\") as handle:\n        for raw_line in handle:\n            line = raw_line.strip()\n            if not line or line.startswith(\"#\"):\n                continue\n            if \"#\" in line:\n                line = line.split(\"#\", 1)[0].strip()\n            if line:\n                requirements.append(line)\n    return requirements\n\n\ndef get_agent_requirements(\n    agent_name: str,\n    agents_dir: str,\n    base_requirements: Optional[List[str]] = None,\n) -> List[str]:\n    \"\"\"Combine base requirements with the agent's requirements.txt.\"\"\"\n    requirements = list(base_requirements or [])\n    req_path = os.path.join(agents_dir, agent_name, \"requirements.txt\")\n    requirements.extend(load_requirements_file(req_path))\n\n    seen = set()\n    deduped = []\n    for req in requirements:\n        if req not in seen:\n            deduped.append(req)\n            seen.add(req)\n    return deduped\n\n\ndef _extract_engine_id(resource_name: Optional[str]) -> Optional[str]:\n    if not resource_name:\n        return None\n    match = re.search(r\"reasoningEngines/([^/]+)\", resource_name)\n    return match.group(1) if match else None\n\n\ndef _resolve_engine_id_from_app(app: Any) -> Optional[str]:\n    resource_name = getattr(app, \"resource_name\", None) or \"\"\n    return _extract_engine_id(resource_name)\n\n\ndef get_deploy_env_vars(\n    agent_name: str,\n    deployed_apps: Optional[Dict[str, Any]] = None,\n) -> Optional[Dict[str, str]]:\n    \"\"\"Return environment variables to inject into the agent runtime.\"\"\"\n    env: Dict[str, str] = {}\n\n    # Required environment variables for Vertex AI\n    if DEPLOY_REGION:\n        env[\"GOOGLE_CLOUD_LOCATION\"] = DEPLOY_REGION\n    env[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"true\"\n    # GOOGLE_CLOUD_PROJECT is reserved; Agent Engine provides it.\n\n    if MODEL_REGION:\n        env[\"VERTEX_AI_MODEL_LOCATION\"] = MODEL_REGION\n    if IMPERSONATE_SERVICE_ACCOUNT:\n        env[\"AIVE_IMPERSONATE_SERVICE_ACCOUNT\"] = IMPERSONATE_SERVICE_ACCOUNT\n\n    if agent_name == \"value_hypothesis_agent\":\n        if OUTPUT_FOLDER_ID:\n            env[\"OUTPUT_FOLDER_ID\"] = OUTPUT_FOLDER_ID\n        if SALES_PLAYS_MD_FOLDER_ID:\n            env[\"SALES_PLAYS_MD_FOLDER_ID\"] = SALES_PLAYS_MD_FOLDER_ID\n\n    # Add agent-specific env vars here if needed.\n\n    return env or None\n\n\ndef _apply_env_vars(\n    fn,\n    kwargs: Dict[str, Any],\n    env_vars: Optional[Dict[str, str]],\n) -> Dict[str, Any]:\n    if not env_vars:\n        return kwargs\n\n    param_name = None\n    for candidate in (\"env_vars\", \"env\", \"environment_variables\", \"environment\"):\n        if candidate in inspect.signature(fn).parameters:\n            param_name = candidate\n            break\n\n    if not param_name:\n        print(\"  ! Warning: agent_engines API does not accept env vars; skipping env injection.\")\n        return kwargs\n\n    kwargs[param_name] = env_vars\n    return kwargs\n\n\ndef _apply_service_account(\n    fn,\n    kwargs: Dict[str, Any],\n    service_account: Optional[str],\n) -> Dict[str, Any]:\n    if not service_account:\n        return kwargs\n\n    param_name = None\n    for candidate in (\"service_account\", \"service_account_email\", \"serviceAccount\"):\n        if candidate in inspect.signature(fn).parameters:\n            param_name = candidate\n            break\n\n    if not param_name:\n        print(\"  ! Warning: agent_engines API does not accept service_account; skipping.\")\n        return kwargs\n\n    kwargs[param_name] = service_account\n    return kwargs\n\n\ndef list_agent_files(package_root: str) -> List[str]:\n    \"\"\"\n    List all Python files and data files in a package root.\n    Returns paths relative to the package root.\n    \"\"\"\n    extra_packages = []\n\n    for root, dirs, files in os.walk(package_root):\n        # Skip __pycache__ and hidden directories\n        dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']\n\n        for file in files:\n            # Skip hidden files and cache files\n            if file.startswith('.') or file.endswith('.pyc'):\n                continue\n\n            # Include Python files, JSON files, JSONL files, and other data files\n            if file.endswith(('.py', '.json', '.jsonl', '.txt', '.yaml', '.yml')):\n                rel_path = os.path.relpath(os.path.join(root, file), package_root)\n                extra_packages.append(rel_path)\n\n    return extra_packages\n\n\ndef prepare_agent_staging(agent_name: str, agents_dir: str) -> str:\n    \"\"\"\n    Prepare staging directory with agent and common/ folder.\n    Returns path to the staging directory.\n    \"\"\"\n    agent_dir = os.path.join(agents_dir, agent_name)\n    common_dir = os.path.join(agents_dir, 'common')\n\n    if not os.path.exists(agent_dir):\n        raise FileNotFoundError(f\"Agent directory not found: {agent_dir}\")\n\n    # Create temp staging directory\n    staging_dir = tempfile.mkdtemp(prefix=f\"adk_build_{agent_name}_\")\n    staged_agent_dir = os.path.join(staging_dir, agent_name)\n\n    # Copy agent directory\n    shutil.copytree(agent_dir, staged_agent_dir)\n\n    # Copy common/ (if exists)\n    if os.path.exists(common_dir):\n        # Keep a copy inside the agent package for local relative imports\n        shutil.copytree(common_dir, os.path.join(staged_agent_dir, 'common'))\n        # Also expose common as a top-level package for runtime imports\n        shutil.copytree(common_dir, os.path.join(staging_dir, 'common'))\n        print(f\"  ✓ Copied common/ directory into agent package and root\")\n    else:\n        print(f\"  ! No common/ directory found at {common_dir}\")\n\n    # Ensure packages so relative imports work\n    init_path = os.path.join(staged_agent_dir, \"__init__.py\")\n    if not os.path.exists(init_path):\n        open(init_path, \"a\").close()\n\n    common_init = os.path.join(staged_agent_dir, \"common\", \"__init__.py\")\n    if os.path.exists(os.path.dirname(common_init)) and not os.path.exists(common_init):\n        open(common_init, \"a\").close()\n\n    root_common_init = os.path.join(staging_dir, \"common\", \"__init__.py\")\n    if os.path.exists(os.path.dirname(root_common_init)) and not os.path.exists(root_common_init):\n        open(root_common_init, \"a\").close()\n\n    return staged_agent_dir\n\n\ndef normalize_extra_packages(package_root: str, extra_packages: List[str]) -> List[str]:\n    \"\"\"Ensure extra_packages are valid paths for the current working dir.\"\"\"\n    normalized = []\n    for rel_path in extra_packages:\n        # Strip leading './' or absolute staging_dir prefix if present\n        rel_path = rel_path.lstrip(\"./\")\n        normalized.append(rel_path)\n    return normalized\n\n\ndef load_agent_module(agent_dir: str, agent_name: str):\n    \"\"\"\n    Dynamically load the agent module from directory.\n    Returns the root_app (AdkApp with greeting) if available, otherwise root_agent.\n    \"\"\"\n    parent_dir = os.path.dirname(agent_dir)\n    for path in [parent_dir, agent_dir]:\n        if path not in sys.path:\n            sys.path.insert(0, path)\n\n    # Clear any cached imports\n    modules_to_remove = [\n        m for m in sys.modules\n        if m == agent_name\n        or m.startswith(f\"{agent_name}.\")\n        or m.startswith('common')\n    ]\n    for m in modules_to_remove:\n        del sys.modules[m]\n\n    # Import as a package so relative imports resolve\n    module = importlib.import_module(f\"{agent_name}.agent\")\n    importlib.reload(module)\n\n    # Prefer root_app (AdkApp with greeting session service) if available\n    if hasattr(module, 'root_app') and module.root_app is not None:\n        print(f\"  ✓ Using root_app (AdkApp with greeting session service)\")\n        return module.root_app\n    else:\n        print(f\"  ! root_app not found, using root_agent\")\n        return module.root_agent\n\n\ndef show_package_sizes(agents_dir: str, agents: List[str]):\n    \"\"\"Show package sizes for agents (helps identify bloat).\"\"\"\n    print(\"\\nAgent package sizes (smaller = faster deploy):\")\n    for agent_name in agents:\n        agent_dir = os.path.join(agents_dir, agent_name)\n        if os.path.exists(agent_dir):\n            # Calculate size\n            total_size = 0\n            for dirpath, dirnames, filenames in os.walk(agent_dir):\n                for f in filenames:\n                    fp = os.path.join(dirpath, f)\n                    total_size += os.path.getsize(fp)\n\n            # Format size\n            if total_size > 1024 * 1024:\n                size_str = f\"{total_size / (1024*1024):.1f}M\"\n            elif total_size > 1024:\n                size_str = f\"{total_size / 1024:.1f}K\"\n            else:\n                size_str = f\"{total_size}B\"\n\n            print(f\"  {agent_name}: {size_str}\")\n        else:\n            print(f\"  {agent_name}: NOT FOUND\")\n    print()\n\n\ndef deploy_agent(\n    agent_name: str,\n    agents_dir: str,\n    requirements: List[str],\n    description: str,\n    env_vars: Optional[Dict[str, str]] = None,\n) -> Any:\n    \"\"\"\n    Deploy a single agent to Vertex AI Agent Engine.\n\n    Steps:\n      1. Prepare staging directory (copy agent + common/)\n      2. Load agent module dynamically\n      3. Collect extra package files\n      4. Create or update a Reasoning Engine\n    Returns the deployed AgentEngine object.\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Deploying: {agent_name}\")\n    print(f\"{'='*60}\")\n\n    # 1. Prepare staging\n    staged_agent_dir = prepare_agent_staging(agent_name, agents_dir)\n    staging_dir = os.path.dirname(staged_agent_dir)\n\n    # 2. Load agent module\n    print(f\"  Loading agent module...\")\n    agent_obj = load_agent_module(staged_agent_dir, agent_name)\n\n    # 3. Collect extra packages (files to upload alongside the agent)\n    extra_packages_files = list_agent_files(staged_agent_dir)\n    # Also include common/ at the staging root if it exists\n    common_staging = os.path.join(staging_dir, \"common\")\n    if os.path.exists(common_staging):\n        for f in list_agent_files(common_staging):\n            extra_packages_files.append(os.path.join(\"..\", \"common\", f))\n\n    print(f\"  Requirements: {requirements}\")\n    print(f\"  Extra package files: {len(extra_packages_files)}\")\n\n    # 4. Determine display name\n    if FORCE_CREATE_NEW_ENGINES:\n        suffix = DISPLAY_NAME_SUFFIX or datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        display_name = f\"{agent_name}_{suffix}\"\n    else:\n        display_name = agent_name\n\n    # 5. Build kwargs for create/update\n    prev_cwd = os.getcwd()\n    os.chdir(staging_dir)\n\n    try:\n        create_kwargs = dict(\n            agent_engine=agent_obj,\n            requirements=requirements,\n            display_name=display_name,\n            description=description,\n        )\n\n        # Add extra_packages if the API supports it\n        if extra_packages_files:\n            create_kwargs[\"extra_packages\"] = normalize_extra_packages(\n                staging_dir, extra_packages_files\n            )\n\n        # Inject env vars\n        create_kwargs = _apply_env_vars(\n            agent_engines.create, create_kwargs, env_vars\n        )\n\n        # Inject service account\n        create_kwargs = _apply_service_account(\n            agent_engines.create, create_kwargs, AGENT_SERVICE_ACCOUNT or None\n        )\n\n        # 6. Check for existing engine (update) or create new\n        if not FORCE_CREATE_NEW_ENGINES:\n            existing_id = get_existing_agent_engine_id(display_name)\n            if existing_id:\n                resource_name = (\n                    f\"projects/{PROJECT_ID}/locations/{DEPLOY_REGION}\"\n                    f\"/reasoningEngines/{existing_id}\"\n                )\n                print(f\"  Updating existing engine: {existing_id}\")\n                try:\n                    app = agent_engines.update(\n                        resource_name=resource_name,\n                        agent_engine=agent_obj,\n                        requirements=requirements,\n                        description=description,\n                        extra_packages=create_kwargs.get(\"extra_packages\"),\n                    )\n                    print(f\"  ✓ Updated: {app.resource_name}\")\n                    return app\n                except Exception as update_err:\n                    print(f\"  ! Update failed: {update_err}\")\n                    print(f\"  Falling back to create...\")\n\n        # Create new engine\n        print(f\"  Creating new engine: {display_name}\")\n        app = agent_engines.create(**create_kwargs)\n        print(f\"  ✓ Created: {app.resource_name}\")\n        return app\n\n    except PermissionDenied as pd_err:\n        resource = extract_engine_resource(str(pd_err))\n        print(f\"  ✗ PermissionDenied during deploy\")\n        if resource:\n            print_engine_log_help(resource)\n        raise\n    except Exception as err:\n        print(f\"  ✗ Deploy failed: {err}\")\n        resource = extract_engine_resource(str(err))\n        if resource:\n            print_engine_log_help(resource)\n        raise\n    finally:\n        os.chdir(prev_cwd)\n        # Clean up staging directory\n        shutil.rmtree(staging_dir, ignore_errors=True)\n\n\ndef deploy_all_agents(\n    agents: List[str],\n    agents_dir: str,\n    requirements: List[str],\n    description: str,\n) -> Dict[str, Any]:\n    \"\"\"\n    Deploy all agents in the list.\n    Returns a dict mapping agent_name -> deployed AgentEngine object.\n    \"\"\"\n    show_package_sizes(agents_dir, agents)\n\n    deployed_apps: Dict[str, Any] = {}\n\n    for agent_name in agents:\n        # Build per-agent requirements\n        if USE_AGENT_REQUIREMENTS:\n            agent_reqs = get_agent_requirements(agent_name, agents_dir, requirements)\n        else:\n            agent_reqs = list(requirements)\n\n        # Build per-agent env vars\n        env_vars = get_deploy_env_vars(agent_name, deployed_apps)\n\n        try:\n            app = deploy_agent(\n                agent_name=agent_name,\n                agents_dir=agents_dir,\n                requirements=agent_reqs,\n                description=description,\n                env_vars=env_vars,\n            )\n            deployed_apps[agent_name] = app\n        except Exception as e:\n            print(f\"\\n  ✗ Failed to deploy {agent_name}: {e}\")\n            print(f\"  Continuing with remaining agents...\\n\")\n\n    # Summary\n    print(f\"\\n{'='*60}\")\n    print(\"Deployment Summary\")\n    print(f\"{'='*60}\")\n    for name in agents:\n        if name in deployed_apps:\n            engine_id = _resolve_engine_id_from_app(deployed_apps[name])\n            print(f\"  ✓ {name} (engine_id={engine_id})\")\n        else:\n            print(f\"  ✗ {name} (FAILED)\")\n    print()\n\n    return deployed_apps\n\n\nprint(\"✓ Helper functions defined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 8. Deploy All Agents (Run This Cell)\n",
    "# Deploy all agents listed in AGENTS_TO_DEPLOY\n",
    "\n",
    "deployed_apps = deploy_all_agents(\n",
    "    agents=AGENTS_TO_DEPLOY,\n",
    "    agents_dir=AGENTS_DIR,\n",
    "    requirements=BASE_REQUIREMENTS,\n",
    "    description=AGENT_DESCRIPTION,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Alternative: Deploy Single Agent\n",
    "\n",
    "Use the cells below to deploy a single agent or test locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title 9. Deploy Single Agent (Optional) { display-mode: \"form\" }\n# @markdown Select which agent to deploy individually\n\nSINGLE_AGENT = \"value_hypothesis_agent\"  # @param [\"value_hypothesis_agent\"]\n\nsingle_requirements = (\n    get_agent_requirements(SINGLE_AGENT, AGENTS_DIR, BASE_REQUIREMENTS)\n    if USE_AGENT_REQUIREMENTS\n    else BASE_REQUIREMENTS\n)\nsingle_env = get_deploy_env_vars(SINGLE_AGENT)\n\nsingle_app = deploy_agent(\n    agent_name=SINGLE_AGENT,\n    agents_dir=AGENTS_DIR,\n    requirements=single_requirements,\n    description=AGENT_DESCRIPTION,\n    env_vars=single_env,\n)\n\nprint(f\"\\nDeployed: {single_app.resource_name}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Testing Deployed Agents"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title 10. Test Agent: Create Session\n# @markdown Test a deployed agent by creating a session\n\nTEST_AGENT = \"value_hypothesis_agent\"  # @param {type:\"string\"}\nTEST_USER_ID = \"test_user_123\"  # @param {type:\"string\"}\n\n# Get the deployed app (either from deploy_all or deploy_single)\nif TEST_AGENT in deployed_apps:\n    test_app = deployed_apps[TEST_AGENT]\nelif 'single_app' in dir() and single_app:\n    test_app = single_app\nelse:\n    # Fetch existing agent\n    existing_id = get_existing_agent_engine_id(TEST_AGENT)\n    if existing_id:\n        resource_name = f\"projects/{PROJECT_ID}/locations/{DEPLOY_REGION}/reasoningEngines/{existing_id}\"\n        test_app = agent_engines.get(resource_name)\n    else:\n        raise ValueError(f\"Agent {TEST_AGENT} not found\")\n\nprint(f\"Testing agent: {test_app.resource_name}\")\n\n# Create session via API\ntoken = get_access_token()\nheaders = {\"Authorization\": f\"Bearer {token}\"}\ndata = {'class_method': 'async_create_session', 'input': {'user_id': TEST_USER_ID}}\nendpoint = f'https://{DEPLOY_REGION}-aiplatform.googleapis.com/v1/{test_app.resource_name}:query'\n\nres = requests.post(endpoint, headers=headers, json=data)\nprint(f\"\\nResponse ({res.status_code}):\")\nprint(res.json())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 11. Test Agent: Send Query\n",
    "# @markdown Send a test message to the agent\n",
    "\n",
    "TEST_MESSAGE = \"hello\"  # @param {type:\"string\"}\n",
    "\n",
    "data = {\n",
    "    'class_method': 'async_stream_query',\n",
    "    'input': {\n",
    "        'user_id': TEST_USER_ID,\n",
    "        'message': TEST_MESSAGE\n",
    "    }\n",
    "}\n",
    "endpoint = f'https://{DEPLOY_REGION}-aiplatform.googleapis.com/v1/{test_app.resource_name}:streamQuery?alt=sse'\n",
    "\n",
    "res = requests.post(endpoint, headers=headers, json=data)\n",
    "print(f\"Response ({res.status_code}):\")\n",
    "print(res.text[:2000] if len(res.text) > 2000 else res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Local Testing with ADK CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title 12. Test Agent Locally (ADK CLI)\n# @markdown Run an agent locally using `adk run`\n\nLOCAL_TEST_AGENT = \"value_hypothesis_agent\"  # @param [\"value_hypothesis_agent\"]\n\nagent_path = os.path.join(AGENTS_DIR, LOCAL_TEST_AGENT)\nprint(f\"Running: adk run {agent_path}\")\nprint(\"Press Ctrl+C to stop\\n\")\n\n!cd {agent_path} && adk run ."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Utility: List All Deployed Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 13. List All Agent Engines in Project\n",
    "\n",
    "token = get_access_token()\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "url = f\"https://{DEPLOY_REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{DEPLOY_REGION}/reasoningEngines\"\n",
    "\n",
    "res = requests.get(url, headers=headers)\n",
    "data = res.json()\n",
    "\n",
    "engines = data.get('reasoningEngines', [])\n",
    "print(f\"Found {len(engines)} Agent Engine(s):\\n\")\n",
    "\n",
    "for engine in engines:\n",
    "    name = engine.get('displayName', 'N/A')\n",
    "    engine_id = engine['name'].split('/')[-1]\n",
    "    created = engine.get('createTime', 'N/A')[:19]\n",
    "    print(f\"  {name}\")\n",
    "    print(f\"    ID: {engine_id}\")\n",
    "    print(f\"    Created: {created}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}